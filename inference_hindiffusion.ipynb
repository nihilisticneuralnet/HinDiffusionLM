{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, random, itertools, math, torch\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer, AutoModelForMaskedLM,\n    get_cosine_schedule_with_warmup\n)\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:36:27.466900Z","iopub.execute_input":"2025-07-27T14:36:27.467052Z","iopub.status.idle":"2025-07-27T14:36:42.540090Z","shell.execute_reply.started":"2025-07-27T14:36:27.467037Z","shell.execute_reply":"2025-07-27T14:36:42.539409Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!huggingface-cli login --token \"your_hf_token\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:37:26.488477Z","iopub.execute_input":"2025-07-27T14:37:26.488770Z","iopub.status.idle":"2025-07-27T14:37:27.104371Z","shell.execute_reply.started":"2025-07-27T14:37:26.488749Z","shell.execute_reply":"2025-07-27T14:37:27.103519Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `mdlm` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `mdlm`\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"model_id = \"<username>/diffusionlm\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nSEP_ID, CLS_ID, MASK_ID = tokenizer.sep_token_id, tokenizer.cls_token_id, tokenizer.mask_token_id\nmodel = AutoModelForMaskedLM.from_pretrained(model_id, device_map=device)\nmodel.eval();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:37:44.560682Z","iopub.execute_input":"2025-07-27T14:37:44.561457Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f83c580ed8495fafd39fcfa64a778c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8166d4a567fd420fbb7e3466037dd78d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc20d2f77e804befb024b89bc0469c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47512424d5648db99c75f25956a6c39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83566c2f64424f8c8504b31a1e03f76c"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Single forward pass:\nprompt = \"User: आप कैसे हैं?\" + tokenizer.sep_token + \" Assistant:\"\nprompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\nans_len = 6\nids = [CLS_ID] + prompt_ids + [SEP_ID] + [MASK_ID]*ans_len + [SEP_ID]\nwith torch.no_grad():\n  outs = model(input_ids=torch.tensor([ids]).to(device)).logits\nprint(outs.shape)\nout_ids = outs[0].argmax(dim=-1).tolist()\nprint(tokenizer.decode(out_ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:50:44.832408Z","iopub.execute_input":"2025-07-27T14:50:44.832680Z","iopub.status.idle":"2025-07-27T14:50:44.854438Z","shell.execute_reply.started":"2025-07-27T14:50:44.832660Z","shell.execute_reply":"2025-07-27T14:50:44.853708Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 18, 197285])\n. Assistant : आप कैसे हैं?. Assistant :. Assistant : आप सहायक हूँ!.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"prompt = \"User: भारत की राजधानी कौनसा है? \" + tokenizer.sep_token + \" Assistant:\"\nprompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\nans_len = 3\nouts = None\nids = [CLS_ID] + prompt_ids + [SEP_ID] + [MASK_ID]*ans_len + [SEP_ID]\nfor i in range(ans_len):\n  if i % 4 == 0: # Optional: only run through the model every 4 (i.e. keep the top 4 each forwrd pass)\n    with torch.no_grad():\n      outs = model(input_ids=torch.tensor([ids]).to(device)).logits\n  out_probs = torch.softmax(outs[0], dim=-1)\n  mask_locs = (torch.tensor(ids) == MASK_ID).nonzero(as_tuple=True)[0]\n  new_probs = torch.zeros_like(out_probs)\n  new_probs[mask_locs] = out_probs[mask_locs]\n  max_probs, max_locs = new_probs.max(dim=-1)\n  max_loc = max_probs.argmax(dim=-1)\n  print(max_loc, tokenizer.decode(new_probs[max_loc].argmax().item()))\n  ids[max_loc] = new_probs[max_loc].argmax().item()\nprint(tokenizer.decode(ids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:52:01.475022Z","iopub.execute_input":"2025-07-27T14:52:01.475395Z","iopub.status.idle":"2025-07-27T14:52:01.501807Z","shell.execute_reply.started":"2025-07-27T14:52:01.475369Z","shell.execute_reply":"2025-07-27T14:52:01.500914Z"}},"outputs":[{"name":"stdout","text":"tensor(14, device='cuda:0') Assistant\ntensor(15, device='cuda:0') :\ntensor(16, device='cuda:0') दिल्ली\n[CLS] User : भारत की राजधानी कौनसा है? [SEP] Assistant : [SEP] Assistant : दिल्ली [SEP]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Wrapping that in a function\ndef sample(q, ans_len=32):\n  prompt = f\"User: {q} \" + tokenizer.sep_token + \" Assistant:\"\n  prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n  ids = [CLS_ID] + prompt_ids + [SEP_ID] + [MASK_ID]*ans_len + [SEP_ID]\n  for i in range(ans_len):\n    with torch.no_grad():\n      outs = model(input_ids=torch.tensor([ids]).to(device)).logits\n    out_probs = torch.softmax(outs[0], dim=-1)\n    mask_locs = (torch.tensor(ids) == MASK_ID).nonzero(as_tuple=True)[0]\n    new_probs = torch.zeros_like(out_probs)\n    new_probs[mask_locs] = out_probs[mask_locs]\n    max_probs, max_locs = new_probs.max(dim=-1)\n    max_loc = max_probs.argmax(dim=-1)\n    ids[max_loc] = new_probs[max_loc].argmax().item()\n  return tokenizer.decode(ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:52:58.662698Z","iopub.execute_input":"2025-07-27T14:52:58.662992Z","iopub.status.idle":"2025-07-27T14:52:58.668995Z","shell.execute_reply.started":"2025-07-27T14:52:58.662973Z","shell.execute_reply":"2025-07-27T14:52:58.668081Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"sample(\"आप कौन हैं\", ans_len=12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T14:54:15.673695Z","iopub.execute_input":"2025-07-27T14:54:15.674029Z","iopub.status.idle":"2025-07-27T14:54:15.876752Z","shell.execute_reply.started":"2025-07-27T14:54:15.674005Z","shell.execute_reply":"2025-07-27T14:54:15.875960Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'[CLS] User : आप कौन हैं [SEP] Assistant : [SEP] Assistant : नमस्ते! मैं कैसे आपकी मदद कर सकता हूँ? [SEP]'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"sample(\"आप कैसे हैं?\", ans_len=8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:07:14.814874Z","iopub.execute_input":"2025-07-27T15:07:14.815525Z","iopub.status.idle":"2025-07-27T15:07:14.925156Z","shell.execute_reply.started":"2025-07-27T15:07:14.815499Z","shell.execute_reply":"2025-07-27T15:07:14.924454Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"'[CLS] User : आप कैसे हैं? [SEP] Assistant : [SEP] Assistant : नमस्ते! मैं कौन हूँ? [SEP]'"},"metadata":{}}],"execution_count":67},{"cell_type":"markdown","source":"## Gradio Interface","metadata":{}},{"cell_type":"code","source":"import os, random, itertools, math, torch\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer, AutoModelForMaskedLM,\n    get_cosine_schedule_with_warmup\n)\nfrom torch.optim import AdamW\nimport torch\nimport numpy as np\nimport gradio as gr\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\nimport time\nimport re\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:09:05.888876Z","iopub.execute_input":"2025-07-27T15:09:05.889142Z","iopub.status.idle":"2025-07-27T15:09:05.894383Z","shell.execute_reply.started":"2025-07-27T15:09:05.889122Z","shell.execute_reply":"2025-07-27T15:09:05.893563Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"!huggingface-cli login --token \"your_hf_token\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model and tokenizer\nmodel_id = \"<username>/diffusionlm\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nSEP_ID, CLS_ID, MASK_ID = tokenizer.sep_token_id, tokenizer.cls_token_id, tokenizer.mask_token_id\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)#, device_map=device)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MASK_TOKEN = \"[MASK]\"\n# MASK_ID = 126336  # The token ID of [MASK] in LLaDA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:13:07.293394Z","iopub.execute_input":"2025-07-27T15:13:07.293728Z","iopub.status.idle":"2025-07-27T15:13:07.297833Z","shell.execute_reply.started":"2025-07-27T15:13:07.293705Z","shell.execute_reply":"2025-07-27T15:13:07.297183Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"def parse_constraints(constraints_text):\n    \"\"\"Parse constraints in format: 'position:word, position:word, ...'\"\"\"\n    constraints = {}\n    if not constraints_text:\n        return constraints\n        \n    parts = constraints_text.split(',')\n    for part in parts:\n        if ':' not in part:\n            continue\n        pos_str, word = part.split(':', 1)\n        try:\n            pos = int(pos_str.strip())\n            word = word.strip()\n            if word and pos >= 0:\n                constraints[pos] = word\n        except ValueError:\n            continue\n    \n    return constraints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:13:08.475218Z","iopub.execute_input":"2025-07-27T15:13:08.475533Z","iopub.status.idle":"2025-07-27T15:13:08.480522Z","shell.execute_reply.started":"2025-07-27T15:13:08.475501Z","shell.execute_reply":"2025-07-27T15:13:08.479671Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"def format_chat_history(history):\n    \"\"\"\n    Format chat history for the LLaDA model\n    \n    Args:\n        history: List of [user_message, assistant_message] pairs\n        \n    Returns:\n        Formatted conversation for the model\n    \"\"\"\n    conversation = \"\"\n    for user_msg, assistant_msg in history:\n        conversation += f\"User: {user_msg} {tokenizer.sep_token} \"\n        if assistant_msg:\n            conversation += f\"Assistant: {assistant_msg} {tokenizer.sep_token} \"\n    return conversation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:13:10.067400Z","iopub.execute_input":"2025-07-27T15:13:10.067696Z","iopub.status.idle":"2025-07-27T15:13:10.072062Z","shell.execute_reply.started":"2025-07-27T15:13:10.067672Z","shell.execute_reply":"2025-07-27T15:13:10.071156Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"def add_gumbel_noise(logits, temperature):\n    '''\n    The Gumbel max is a method for sampling categorical distributions.\n    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.\n    Thus, we use float64.\n    '''\n    if temperature <= 0:\n        return logits\n        \n    logits = logits.to(torch.float64)\n    noise = torch.rand_like(logits, dtype=torch.float64)\n    gumbel_noise = (- torch.log(noise)) ** temperature\n    return logits.exp() / gumbel_noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:13:10.297750Z","iopub.execute_input":"2025-07-27T15:13:10.297990Z","iopub.status.idle":"2025-07-27T15:13:10.302481Z","shell.execute_reply.started":"2025-07-27T15:13:10.297973Z","shell.execute_reply":"2025-07-27T15:13:10.301704Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"def get_num_transfer_tokens(mask_index, steps):\n    '''\n    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.\n    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),\n    the expected number of tokens transitioned at each step should be consistent.\n\n    This function is designed to precompute the number of tokens that need to be transitioned at each step.\n    '''\n    mask_num = mask_index.sum(dim=1, keepdim=True)\n\n    base = mask_num // steps\n    remainder = mask_num % steps\n\n    num_transfer_tokens = torch.zeros(mask_num.size(0), steps, device=mask_index.device, dtype=torch.int64) + base\n\n    for i in range(mask_num.size(0)):\n        num_transfer_tokens[i, :remainder[i]] += 1\n\n    return num_transfer_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:13:11.991453Z","iopub.execute_input":"2025-07-27T15:13:11.991721Z","iopub.status.idle":"2025-07-27T15:13:11.996989Z","shell.execute_reply.started":"2025-07-27T15:13:11.991698Z","shell.execute_reply":"2025-07-27T15:13:11.995981Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"def generate_response_with_visualization(model, tokenizer, device, history, gen_length=32, \n                                                   constraints=None, recompute_every=4):\n    \"\"\"\n    Generate text with LLaDA model with visualization using the same sampling as in generate.py\n    \n    Args:\n        messages: List of message dictionaries with 'role' and 'content'\n        gen_length: Length of text to generate\n        steps: Number of denoising steps\n        constraints: Dictionary mapping positions to words\n        temperature: Sampling temperature\n        cfg_scale: Classifier-free guidance scale\n        block_length: Block length for semi-autoregressive generation\n        remasking: Remasking strategy ('low_confidence' or 'random')\n        \n    Returns:\n        List of visualization states showing the progression and final text\n    \"\"\"\n    if not history:\n        return [], \"\"\n    \n    last_user_message = history[-1][0]\n    conversation_history = format_chat_history(history[:-1])\n    \n    prompt = conversation_history + f\"User: {last_user_message} {tokenizer.sep_token} Assistant:\"\n    prompt_ids = tokenizer.encode(prompt, add_special_tokens=False)\n    \n    ids = [CLS_ID] + prompt_ids + [SEP_ID] + [MASK_ID] * gen_length + [SEP_ID]\n    \n    visualization_states = []\n    initial_state = [(tokenizer.mask_token, \"#444444\") for _ in range(gen_length)]\n    visualization_states.append(initial_state)\n    \n    old_ids = ids.copy()\n    outs = None\n    \n    for i in range(gen_length):\n        if i % recompute_every == 0:\n            with torch.no_grad():\n                outs = model(input_ids=torch.tensor([ids]).to(device)).logits\n        \n        out_probs = torch.softmax(outs[0], dim=-1)\n        mask_locs = (torch.tensor(ids) == MASK_ID).nonzero(as_tuple=True)[0]\n        \n        if len(mask_locs) == 0:\n            break\n            \n        new_probs = torch.zeros_like(out_probs)\n        new_probs[mask_locs] = out_probs[mask_locs]\n        \n        max_probs, max_locs = new_probs.max(dim=-1)\n        max_loc = max_probs.argmax(dim=-1)\n        \n        old_ids = ids.copy()\n        ids[max_loc] = new_probs[max_loc].argmax().item()\n        \n        current_state = []\n        response_start_idx = len([CLS_ID] + prompt_ids + [SEP_ID])\n        \n        for j in range(gen_length):\n            pos = response_start_idx + j\n            \n            if ids[pos] == MASK_ID:\n                current_state.append((tokenizer.mask_token, \"#444444\"))\n            elif old_ids[pos] == MASK_ID:\n                token = tokenizer.decode([ids[pos]], skip_special_tokens=True)\n                confidence = float(max_probs[max_loc].cpu())\n                if confidence < 0.3:\n                    color = \"#FF6666\"\n                elif confidence < 0.7:\n                    color = \"#FFAA33\"\n                else:\n                    color = \"#66CC66\"\n                current_state.append((token, color))\n            else:\n                token = tokenizer.decode([ids[pos]], skip_special_tokens=True)\n                current_state.append((token, \"#6699CC\"))\n        \n        visualization_states.append(current_state)\n    \n    response_start_idx = len([CLS_ID] + prompt_ids + [SEP_ID])\n    response_end_idx = response_start_idx + gen_length\n    response_ids = ids[response_start_idx:response_end_idx]\n    response_ids = [token_id for token_id in response_ids if token_id != MASK_ID]\n    final_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n    \n    return visualization_states, final_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:13:14.849573Z","iopub.execute_input":"2025-07-27T15:13:14.850199Z","iopub.status.idle":"2025-07-27T15:13:14.867058Z","shell.execute_reply.started":"2025-07-27T15:13:14.850162Z","shell.execute_reply":"2025-07-27T15:13:14.866189Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"def bot_response(history, gen_length, delay, recompute_every):\n    if not history:\n        return history, [], \"\"\n        \n    try:\n        vis_states, response_text = generate_response_with_visualization(\n            model, tokenizer, device, \n            history, \n            gen_length=gen_length,\n            recompute_every=recompute_every\n        )\n        \n        history[-1][1] = response_text\n        \n        yield history, vis_states[0], response_text\n        \n        for state in vis_states[1:]:\n            time.sleep(delay)\n            yield history, state, response_text\n            \n    except Exception as e:\n        error_msg = f\"Error: {str(e)}\"\n        print(error_msg)\n        \n        error_vis = [(error_msg, \"red\")]\n        \n        yield history, error_vis, error_msg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:14:11.018129Z","iopub.execute_input":"2025-07-27T15:14:11.018892Z","iopub.status.idle":"2025-07-27T15:14:11.024293Z","shell.execute_reply.started":"2025-07-27T15:14:11.018864Z","shell.execute_reply":"2025-07-27T15:14:11.023473Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"def create_settings():\n    with gr.Accordion(\"Generation Settings\", open=False):\n        with gr.Row():\n            gen_length = gr.Slider(\n                minimum=8, maximum=64, value=32, step=4,\n                label=\"Generation Length\"\n            )\n            recompute_every = gr.Slider(\n                minimum=1, maximum=8, value=4, step=1,\n                label=\"Recompute Logits Every N Steps\"\n            )\n        with gr.Row():\n            visualization_delay = gr.Slider(\n                minimum=0.0, maximum=1.0, value=0.1, step=0.1,\n                label=\"Visualization Delay (seconds)\"\n            )\n    \n    return gen_length, recompute_every, visualization_delay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:14:12.059642Z","iopub.execute_input":"2025-07-27T15:14:12.060204Z","iopub.status.idle":"2025-07-27T15:14:12.064804Z","shell.execute_reply.started":"2025-07-27T15:14:12.060181Z","shell.execute_reply":"2025-07-27T15:14:12.064147Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"css = '''\n.category-legend{display:none}\nbutton{height: 60px}\n'''\ndef create_chatbot_demo():\n    with gr.Blocks(css=css) as demo:\n        gr.Markdown(\"# Diffusion Language Model Demo\")\n        gr.Markdown(\"Interactive demo showing the denoising process of the diffusion language model\")\n        \n        chat_history = gr.State([])\n        \n        with gr.Row():\n            with gr.Column(scale=3):\n                chatbot_ui = gr.Chatbot(label=\"Conversation\", height=500)\n                \n                with gr.Group():\n                    with gr.Row():\n                        user_input = gr.Textbox(\n                            label=\"Your Message\", \n                            placeholder=\"Type your message here...\",\n                            show_label=False\n                        )\n                        send_btn = gr.Button(\"Send\")\n                        \n            with gr.Column(scale=2):\n                output_vis = gr.HighlightedText(\n                    label=\"Denoising Process Visualization\",\n                    combine_adjacent=False,\n                    show_legend=True,\n                )\n        \n        with gr.Accordion(\"Generation Settings\", open=False):\n            with gr.Row():\n                gen_length = gr.Slider(\n                    minimum=8, maximum=64, value=32, step=4,\n                    label=\"Generation Length\"\n                )\n                recompute_every = gr.Slider(\n                    minimum=1, maximum=8, value=4, step=1,\n                    label=\"Recompute Logits Every N Steps\"\n                )\n            with gr.Row():\n                visualization_delay = gr.Slider(\n                    minimum=0.0, maximum=1.0, value=0.1, step=0.1,\n                    label=\"Visualization Delay (seconds)\"\n                )\n        \n        clear_btn = gr.Button(\"Clear Conversation\")\n        \n        def add_message(history, message, response):\n            history = history.copy()\n            history.append([message, response])\n            return history\n            \n        def user_message_submitted(message, history):\n            if not message.strip():\n                return history, history, \"\"\n                \n            history = add_message(history, message, None)\n            history_for_display = history.copy()\n            message_out = \"\"\n            \n            return history, history_for_display, message_out\n            \n        def bot_response(history, gen_length, delay, recompute_every):\n            if not history:\n                return history, [], \"\"\n                \n            try:\n                vis_states, response_text = generate_response_with_visualization(\n                    model, tokenizer, device, \n                    history, \n                    gen_length=gen_length,\n                    recompute_every=recompute_every\n                )\n                \n                history[-1][1] = response_text\n                \n                yield history, vis_states[0], response_text\n                \n                for state in vis_states[1:]:\n                    time.sleep(delay)\n                    yield history, state, response_text\n                    \n            except Exception as e:\n                error_msg = f\"Error: {str(e)}\"\n                print(error_msg)\n                error_vis = [(error_msg, \"red\")]\n                yield history, error_vis, error_msg\n        \n        def clear_conversation():\n            return [], [], []\n        \n        clear_btn.click(\n            fn=clear_conversation,\n            inputs=[],\n            outputs=[chat_history, chatbot_ui, output_vis]\n        )\n        \n        msg_submit = user_input.submit(\n            fn=user_message_submitted,\n            inputs=[user_input, chat_history],\n            outputs=[chat_history, chatbot_ui, user_input]\n        )\n        \n        send_click = send_btn.click(\n            fn=user_message_submitted,\n            inputs=[user_input, chat_history],\n            outputs=[chat_history, chatbot_ui, user_input]\n        )\n        \n        msg_submit.then(\n            fn=bot_response,\n            inputs=[chat_history, gen_length, visualization_delay, recompute_every],\n            outputs=[chatbot_ui, output_vis]\n        )\n        \n        send_click.then(\n            fn=bot_response,\n            inputs=[chat_history, gen_length, visualization_delay, recompute_every],\n            outputs=[chatbot_ui, output_vis]\n        )\n        \n    return demo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:20:40.155363Z","iopub.execute_input":"2025-07-27T15:20:40.155943Z","iopub.status.idle":"2025-07-27T15:20:40.167339Z","shell.execute_reply.started":"2025-07-27T15:20:40.155919Z","shell.execute_reply":"2025-07-27T15:20:40.166642Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"demo = create_chatbot_demo()\ndemo.queue().launch(share=False) # To create a public link, set `share=True`","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-27T15:39:33.578739Z","iopub.execute_input":"2025-07-27T15:39:33.579311Z","iopub.status.idle":"2025-07-27T15:39:33.912974Z","shell.execute_reply.started":"2025-07-27T15:39:33.579271Z","shell.execute_reply":"2025-07-27T15:39:33.912157Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1658756857.py:16: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot_ui = gr.Chatbot(label=\"Conversation\", height=500)\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7863\n* To create a public link, set `share=True` in `launch()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":84,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}